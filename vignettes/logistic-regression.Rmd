---
title: "Training a logistic regression from Twitter comments"
author: "Jorge Silva"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Training a logistic from Twitter comments}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Training a logistic regression from Twitter comments

In this example, we'll train a logistic regression to classify tweets using only
the natural language text found in these tweets. We'll only need about 800 tweets
per user account.

### Setup

To run this example, you will need the following packages.

```
install.packages("basilica", "dplyr", "stats")
```

## Step 1: Embeddings all tweets

We'll use a collection of about 800 tweets from Bill Gates and Kanye West and
train a logistic regression to predict (given a tweet) which account the tweet
belongs to. In order to do that, we'll first load the tweets from the basilica
package.

    library(jsonlite)
    bill <- fromJSON(system.file("extdata/twitter/billgates.json", package="basilica"))
    kanye <- fromJSON(system.file("extdata/twitter/kanyewest.json", package="basilica"))

Now that we've loaded the JSON files, we can embedded the text of these tweets
using Basilica.

    library(basilica)
    connect("28c4dab5-27f1-52a7-508e-527dd234224c", "http://api.localhost:8000")

    embeddings <- rbind(embed_sentences(bill[, 7]), embed_sentences(kanye[, 7])) # 7 is the index of the text

## Step 2: Running PCA + Cleaning Data

Now that we have these embeddings, we'll want to run PCA and get the 100 features
that explain the most variance. We'll also add a column to the matrix with the 
corresponding category each tweet belongs to.

    pca <- prcomp(t(embeddings), center = TRUE,scale = TRUE)
    features <- pca$rotation[,1:100]

    type <- c(integer(dim(bill)[1]) + 1, integer(dim(kanye)[1]))
    features <- cbind(type, features)
    features <- data.frame(features[sample.int(nrow(features)),])

## Step 3: Training the model

Finally, we can now train our model. In order to do that we'll separate out the
data into training and test data.

    library(dplyr)
    train_data <- sample_frac(features, 0.8)
    train_index <- as.numeric(rownames(train_data))
    test_data <- features[-train_index, ]

    library('stats')
    model <- lm(formula = type ~ ., data = train_data)

## Step 4: Verifying Results

After training the model, we can verify who well it's trained by getting the R 
squared and calculating the correlation accuracy.

    r_squared <- summary(model)$r.squared
    print(r_squared)

    predictions <- predict(model, test_data)
    actuals_preds <- data.frame(cbind(actuals=test_data$type, predicteds=predictions))
    correlation_accuracy <- cor(actuals_preds)
    print(correlation_accuracy)

You have now trained a logistic regression with only the natural language text
of the tweets and 800 data points per category and getting an R squared of about `0.80`.
